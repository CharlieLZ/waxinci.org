#!/usr/bin/env python3
"""
ç²¾ç®€ç‰ˆGoogle Trendsè„šæœ¬ - åªå¤„ç†RisingæŸ¥è¯¢
ä¸ºç§å­å…³é”®è¯æ·»åŠ å›ºå®šçš„gptså€¼ï¼Œåªè¿”å›risingå­—æ®µæ•°æ®
"""

import json
import requests
import base64
import time
import logging
from datetime import datetime
from pathlib import Path
from urllib.parse import quote_plus

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('rising_only_trends.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# APIé…ç½®
API_LOGIN = "chen8mei@yeah.net"
API_PASSWORD = "128216638d1a29af"
API_BASE_URL = "https://api.dataforseo.com/v3/keywords_data/google_trends/explore"

# ç§å­å…³é”®è¯çš„å›ºå®šGPTSå€¼è®¾ç½®
SEED_KEYWORDS_GPTS = {
    "generator": 85,
    "creator": 78,
    "maker": 82,
    "builder": 75,
    "constructor": 70,
    "composer": 65,
    "helper": 88,
    "assistant": 92,
    "agent": 80,
    "advisor": 77,
    "tool": 95,
    "directory": 60,
    "top": 90,
    "best": 85,
    "list": 75,
    "portal": 55,
    "finder": 70,
    "example": 65,
    "template": 80,
    "sample": 70,
    "pattern": 68,
    "resources": 72,
    "guide": 85,
    "format": 60,
    "model": 88,
    "layout": 65,
    "ideas": 78,
    "starter": 70,
    "cataloger": 50,
    "dashboard": 82,
    "designer": 90,
    "uploader": 75,
    "downloader": 78,
    "scraper": 85,
    "crawler": 80,
    "syncer": 55,
    "translator": 88,
    "converter": 82,
    "editor": 90,
    "optimizer": 85,
    "enhancer": 75,
    "modifier": 70,
    "processor": 82,
    "compiler": 78,
    "analyzer": 85,
    "evaluator": 72,
    "calculator": 88,
    "online": 95,
    "checker": 82,
    "detector": 78,
    "humanizer": 60,
    "tester": 75,
    "planner": 82,
    "scheduler": 78,
    "manager": 88,
    "tracker": 85,
    "sender": 65,
    "receiver": 68,
    "responder": 70,
    "recorder": 75,
    "connector": 72,
    "viewer": 80,
    "monitor": 85,
    "notifier": 75,
    "verifier": 78,
    "simulator": 82,
    "comparator": 75,
    "answer": 90,
    "hint": 70,
    "clue": 68,
    "cheat": 75,
    "solver": 88,
    "extractor": 82,
    "summarizer": 85,
    "transcriber": 78,
    "paraphaser": 70,
    "writer": 92,
    "image": 95,
    "photo": 90,
    "picture": 88,
    "face": 85,
    "emoji": 80,
    "meme": 85,
    "chart": 82,
    "graph": 78,
    "style": 88,
    "filter": 85,
    "text": 92,
    "chat": 95,
    "code": 90,
    "video": 95,
    "audio": 88,
    "voice": 85,
    "sound": 82,
    "speech": 80,
    "song": 88,
    "music": 90,
    "how to": 98,
    "icon": 78,
    "logo": 85,
    "avatar": 82,
    "anime": 88,
    "portrait": 80,
    "product photo": 85,
    "cartoon": 85,
    "tattoo": 82,
    "character": 88,
    "coloring page": 75,
    "action": 85,
    "figure": 78,
    "diagram": 80,
    "font": 88,
    "illustration": 85,
    "interior design": 82,
    "upscaler": 78
}

def get_auth_header():
    """è·å–è®¤è¯å¤´"""
    credentials = f"{API_LOGIN}:{API_PASSWORD}"
    encoded_credentials = base64.b64encode(credentials.encode()).decode()
    return {"Authorization": f"Basic {encoded_credentials}", "Content-Type": "application/json"}

def generate_google_trends_link(query, time_range="past_7_days", language="en"):
    """ç”ŸæˆGoogle Trendsé“¾æ¥"""
    base_url = "https://trends.google.com/trends/explore"
    
    # æ—¶é—´èŒƒå›´æ˜ å°„
    time_mapping = {
        "past_hour": "now%201-H",
        "past_4_hours": "now%204-H", 
        "past_day": "now%201-d",
        "past_7_days": "now%207-d",
        "past_30_days": "today%201-m",
        "past_90_days": "today%203-m",
        "past_year": "today%2012-m",
        "past_5_years": "today%205-y"
    }
    
    # æ„å»ºURLå‚æ•°
    params = {
        "hl": language,
        "date": time_mapping.get(time_range, "now%207-d"),
        "q": query
    }
    
    # æ„å»ºå®Œæ•´URL
    param_string = "&".join([f"{k}={quote_plus(str(v))}" for k, v in params.items()])
    full_url = f"{base_url}?{param_string}"
    
    return full_url

def load_keywords_from_csv(limit=None):
    """ä»CSVæ–‡ä»¶åŠ è½½å…³é”®è¯"""
    csv_path = Path("keywords_list.csv")
    if not csv_path.exists():
        logger.error(f"æœªæ‰¾åˆ°å…³é”®è¯æ–‡ä»¶: {csv_path}")
        return []
    
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]
        
        keywords = [line.strip('"\'') for line in lines if line.strip()]
        if limit:
            keywords = keywords[:limit]
        
        logger.info(f"æˆåŠŸè¯»å– {len(keywords)} ä¸ªå…³é”®è¯")
        return keywords
    except Exception as e:
        logger.error(f"è¯»å–å…³é”®è¯æ–‡ä»¶å¤±è´¥: {str(e)}")
        return []

def post_batch_tasks(keywords, time_range="past_7_days"):
    """æ‰¹é‡æäº¤ä»»åŠ¡"""
    tasks = []
    for keyword in keywords:
        task = {
            "keywords": [keyword],
            "time_range": time_range,
            "type": "web",
            "item_types": ["google_trends_queries_list"],
            "language_code": "en",
            "tag": keyword
        }
        tasks.append(task)
    
    url = f"{API_BASE_URL}/task_post"
    headers = get_auth_header()
    
    try:
        logger.info(f"æ‰¹é‡æäº¤ä»»åŠ¡ï¼ŒåŒ…å« {len(keywords)} ä¸ªå…³é”®è¯")
        response = requests.post(url, headers=headers, json=tasks, timeout=60)
        
        if response.status_code == 200:
            data = response.json()
            if data.get("status_code") == 20000:
                task_ids = [task.get("id") for task in data.get("tasks", []) if task.get("id")]
                logger.info(f"âœ… æˆåŠŸæäº¤ {len(task_ids)} ä¸ªä»»åŠ¡")
                return task_ids
            else:
                logger.error(f"APIé”™è¯¯: {data.get('status_message', 'æœªçŸ¥é”™è¯¯')}")
        else:
            logger.error(f"HTTPé”™è¯¯: {response.status_code}")
    
    except Exception as e:
        logger.error(f"æäº¤ä»»åŠ¡å¼‚å¸¸: {str(e)}")
    
    return []

def wait_for_completion(task_ids, max_wait=1800):
    """ç­‰å¾…ä»»åŠ¡å®Œæˆ"""
    logger.info(f"ç­‰å¾… {len(task_ids)} ä¸ªä»»åŠ¡å®Œæˆ...")
    
    completed_ids = set()
    start_time = time.time()
    
    while len(completed_ids) < len(task_ids):
        if time.time() - start_time > max_wait:
            logger.warning("ç­‰å¾…è¶…æ—¶")
            break
        
        try:
            url = f"{API_BASE_URL}/tasks_ready"
            headers = get_auth_header()
            response = requests.get(url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                if data.get("status_code") == 20000:
                    for entry in data.get("tasks", []):
                        for task in entry.get("result", []):
                            task_id = task.get("id")
                            if task_id in task_ids and task_id not in completed_ids:
                                completed_ids.add(task_id)
            
            if len(completed_ids) >= len(task_ids):
                break
                
            logger.info(f"å·²å®Œæˆ {len(completed_ids)}/{len(task_ids)} ä¸ªä»»åŠ¡ï¼Œç­‰å¾…30ç§’åç»§ç»­æ£€æŸ¥...")
            time.sleep(30)
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å¼‚å¸¸: {str(e)}")
            time.sleep(30)
    
    logger.info(f"æœ€ç»ˆå®Œæˆ {len(completed_ids)} ä¸ªä»»åŠ¡")
    return list(completed_ids)

def get_task_results(task_ids, time_range="past_7_days"):
    """è·å–ä»»åŠ¡ç»“æœï¼Œåªå¤„ç†risingå­—æ®µ"""
    logger.info(f"è·å– {len(task_ids)} ä¸ªä»»åŠ¡çš„ç»“æœ...")
    
    results = {}
    headers = get_auth_header()
    
    for task_id in task_ids:
        try:
            url = f"{API_BASE_URL}/task_get/{task_id}"
            response = requests.get(url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                if data.get("status_code") == 20000:
                    tasks = data.get("tasks", [])
                    if tasks:
                        task = tasks[0]
                        if task.get("status_code") == 20000:
                            keyword = task.get("data", {}).get("tag", "")
                            result = task.get("result", [])
                            
                            if keyword and result:
                                results[keyword] = parse_rising_only(result, keyword, time_range)
                                logger.info(f"è·å–ç»“æœæˆåŠŸ: {keyword}")
        
        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡ç»“æœå¼‚å¸¸ [{task_id}]: {str(e)}")
    
    logger.info(f"æˆåŠŸè·å– {len(results)} ä¸ªå…³é”®è¯çš„ç»“æœ")
    return results

def parse_rising_only(result, original_keyword, time_range="past_7_days"):
    """åªè§£ærisingå­—æ®µï¼Œä¸ºç§å­å…³é”®è¯æ·»åŠ å›ºå®šgptså€¼"""
    # ç§å­å…³é”®è¯æ•°æ®
    seed_data = {
        "keyword": original_keyword,
        "gpts": SEED_KEYWORDS_GPTS.get(original_keyword.lower(), 75),  # é»˜è®¤75
        "rising_queries": []
    }
    
    # è§£æAPIè¿”å›çš„risingæŸ¥è¯¢
    for item in result:
        if not isinstance(item, dict):
            continue
        
        for sub_item in item.get("items", []):
            if sub_item.get("type") == "google_trends_queries_list":
                data = sub_item.get("data", {})
                
                # åªå¤„ç†risingæŸ¥è¯¢
                if "rising" in data:
                    for i, query in enumerate(data["rising"][:10], 1):
                        query_text = query.get("query", "")
                        growth_value = query.get("value", 0)
                        
                        parsed_query = {
                            "query": query_text,
                            "growth_rate": growth_value,
                            "growth_rate_formatted": format_growth_rate(growth_value),
                            "rank": i,
                            "link": generate_google_trends_link(query_text, time_range)
                        }
                        seed_data["rising_queries"].append(parsed_query)
    
    return seed_data

def format_growth_rate(value):
    """æ ¼å¼åŒ–å¢é•¿ç‡"""
    if value == "BREAKOUT":
        return "BREAKOUT"
    
    try:
        numeric_value = float(value)
        return f"+{numeric_value:.0f}%"
    except (ValueError, TypeError):
        return "0%"

def save_rising_only_results(results, keywords, time_range="past_7_days"):
    """ä¿å­˜åªåŒ…å«risingå­—æ®µçš„ç»“æœ"""
    # ç»Ÿè®¡ä¿¡æ¯
    total_rising_queries = sum(len(data.get("rising_queries", [])) for data in results.values())
    keywords_with_rising = len([k for k, v in results.items() if v.get("rising_queries")])
    
    # æŒ‰gptså€¼æ’åºç§å­å…³é”®è¯
    sorted_keywords = sorted(results.items(), key=lambda x: x[1].get("gpts", 0), reverse=True)
    
    output_data = {
        "metadata": {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "timestamp": datetime.now().isoformat(),
            "time_range": time_range,
            "api_info": {
                "rising_value_meaning": "å¢é•¿ç‡ç™¾åˆ†æ¯” (æ•´æ•°) æˆ– BREAKOUT (çªç ´æ€§å¢é•¿)",
                "gpts_meaning": "Google Trends è¶‹åŠ¿åˆ†æ•° (0-100)ï¼Œæ•°å€¼è¶Šé«˜è¡¨ç¤ºè¶Šçƒ­é—¨"
            },
            "description": "åªåŒ…å«RisingæŸ¥è¯¢çš„Google Trendsæ•°æ®"
        },
        "summary": {
            "total_seed_keywords": len(keywords),
            "processed_keywords": len(results),
            "total_rising_queries": total_rising_queries,
            "keywords_with_rising": keywords_with_rising,
            "completion_rate": f"{len(results)/len(keywords)*100:.1f}%" if keywords else "0%",
            "avg_gpts": f"{sum(data.get('gpts', 0) for data in results.values()) / len(results):.1f}" if results else "0"
        },
        "seed_keywords": {
            keyword: {
                "gpts": data["gpts"],
                "rising_queries_count": len(data["rising_queries"]),
                "rising_queries": data["rising_queries"]
            }
            for keyword, data in sorted_keywords
        }
    }
    
    output_file = f"rising_only_trends_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
        return ""

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹Rising-only Google Trendsæ•°æ®è·å–...")
    
    # 1. åŠ è½½å…³é”®è¯ï¼ˆæµ‹è¯•ç”¨10ä¸ªï¼‰
    keywords = load_keywords_from_csv(limit=10)
    if not keywords:
        logger.error("æ²¡æœ‰å…³é”®è¯ï¼Œç¨‹åºé€€å‡º")
        return
    
    # 2. æ˜¾ç¤ºç§å­å…³é”®è¯çš„å›ºå®šGPTSå€¼
    logger.info("ğŸ“Š ç§å­å…³é”®è¯å›ºå®šGPTSå€¼:")
    for keyword in keywords:
        gpts = SEED_KEYWORDS_GPTS.get(keyword.lower(), 75)
        logger.info(f"   {keyword}: {gpts}")
    
    # 3. æ‰¹é‡æäº¤ä»»åŠ¡
    time_range = "past_7_days"
    task_ids = post_batch_tasks(keywords, time_range)
    if not task_ids:
        logger.error("ä»»åŠ¡æäº¤å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # 4. ç­‰å¾…ä»»åŠ¡å®Œæˆ
    completed_ids = wait_for_completion(task_ids)
    
    # 5. è·å–ç»“æœï¼ˆåªå¤„ç†risingå­—æ®µï¼‰
    results = get_task_results(completed_ids, time_range)
    
    # 6. ä¿å­˜ç»“æœ
    output_file = save_rising_only_results(results, keywords, time_range)
    
    # 7. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    logger.info("ğŸ“Š å¤„ç†å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯:")
    logger.info(f"   ğŸ¯ å…³é”®è¯æ€»æ•°: {len(keywords)}")
    logger.info(f"   ğŸ“¤ æäº¤ä»»åŠ¡: {len(task_ids)}")
    logger.info(f"   âœ… å®Œæˆä»»åŠ¡: {len(completed_ids)}")
    logger.info(f"   ğŸ“Š è·å–ç»“æœ: {len(results)}")
    logger.info(f"   ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # æ˜¾ç¤ºç¤ºä¾‹æ•°æ®
    if results:
        logger.info("ğŸ” ç»“æœç¤ºä¾‹:")
        for keyword, data in list(results.items())[:3]:
            gpts = data.get("gpts", 0)
            rising_count = len(data.get("rising_queries", []))
            logger.info(f"   {keyword} (GPTS: {gpts}): {rising_count} ä¸ªrisingæŸ¥è¯¢")
            
            # æ˜¾ç¤ºå‰2ä¸ªrisingæŸ¥è¯¢
            for query in data.get("rising_queries", [])[:2]:
                logger.info(f"     - {query['query']} ({query['growth_rate_formatted']})")

if __name__ == "__main__":
    main() 